
DATA_PATH = 'data/'

# Inference Parameters
MAX_NEW_TOKENS = 1250
TEMPERATURE= 0.0
CONTEXT_SIZE= 1500

# Retrieval
EMBEDDING_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'
RETURN_SOURCE_DOCUMENTS = True
VECTOR_COUNT = 3
CHUNK_SIZE = 400
CHUNK_OVERLAP = 0

# Model
MODEL_TYPE = 'openai' # (e.g, local, openai, huggingface)
# If you are using OpenAI APIs (it is enabled if MODEL_TYPE='openai')
OPENAI_API_KEY = ""

# If you are using huggingface endpoints (it is enabled if MODEL_TYPE='huggingface')
HUGGING_FACE_API_URL = ''
HUGGING_FACE_HUB_TOKEN = ''

# If you are using a bin model and you want to load that in your host machine locally (it is enabled if MODEL_TYPE='local')
MODEL_PATH = 'models/'
MODEL_URL_PATH = "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin"